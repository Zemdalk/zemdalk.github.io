---
title: 概率论与数理统计（3）——参数估计
tags: A-课程笔记 概率论与数理统计
mathjax: true
mathjax_autoNumber: true
key: 参数估计
sharing: true
typora-root-url: ..
---

> 概率论与数理统计第七章——参数估计

<!--more-->

<style>
    .spoiler {
        color: transparent;
        text-shadow: 0 0 10px rgba(0, 0, 0, 0.4);
        transition: all 0.4s;
        cursor: pointer;
        position: relative;
    }
    .after {
        opacity: 1;
    }
</style>

# 第七章  参数估计

> - 参数估计与非参数估计
> 
> 参数估计：总体分布类型已知，通过样本估计其中的未知参数；
>
> 非参数估计：总体分布类型未知，通过样本估计总体的分布。

## 7.1  点估计

### 7.1.1  问题的引入

> 设$X_1,X_2,\cdots,X_n$是总体$X$中的一批样本，$X$服从正态分布，但是不知道期望。该如何估计期望呢？我们就从这一批样本出发，建立一个统计量，或者说寻求一个样本的函数，用它的值来近似估计期望，这就是点估计。

- <u>定义</u>

对未知参数进行定值估计的问题就是点估计问题。

- <u>估计量与估计值</u>

构造一个统计量$\hat \theta (X_1,X_2,...,X_n)$，把样本的观察值代入，得到这个统计量的观察值$\hat \theta (x_1,x_2,...,x_n)$，用这个观察值来估计未知参数$\theta$，称前者为估计量，后者为估计值。

- <u>常用构造估计值的方法</u>

矩估计法、最大似然估计法、Bayes估计法

### 7.1.2  矩估计法

> 先看一道题目，直接看解法，体会一下方法。

**<u>例1</u>**  设总体$X$服从参数为$a,b$的均匀分布，即$X\sim U[a,b]$，参数$a,b$未知。从中得到一批样本$X_1,X_2,...,X_n$，求$a,b$的矩估计量。

**解**：总体的$l$阶矩为：

$$
\mu _l=E[X^l]
$$

于是

$$
\mu _1=E[X]=\frac{a+b}{2}
$$

$$
\mu _2=E[X^2]=\frac{(b-a)^2}{12}+\frac{(a+b)^2}{4}（为什么？）
$$

令样本的各阶矩与总体的各阶矩相等，不妨设样本的$l$阶矩为$A_l$，则

$$
A_1=\frac{a+b}{2}
$$

$$
A_2=\frac{(b-a)^2}{12}+\frac{(a+b)^2}{4}
$$

解得

$$
\hat a=A_1-\sqrt{3(A_2-A_1^2)}=\bar{X}-\sqrt{\frac{3}{n}\sum_{i=1}^n(X_i-\bar X)^2}
$$

$$
\hat b=A_1+\sqrt{3(A_2-A_1^2)}=\bar{X}+\sqrt{\frac{3}{n}\sum_{i=1}^n(X_i-\bar X)^2}
$$

*注：这里用到了对样本方差的有偏估计，即$A_2-A_1^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$。*

<u>总结</u>：
矩估计法求估计量的步骤：

1. 求$\mu_i=E[X^i],i=1,2,...$；
2. 设$A_i=\mu_i$；
3. 解上面的方程组，得各参数的矩估计。

### 7.1.3  极大似然估计法

<!-- <span class="spoiler" onMouseUp="this.className='after'">
My hidden paragraph here. <br>
This is for test.<br>
Never misjudge.
</span> -->

> 对于一次随机试验，假设它有$A_1,A_2,...,A_m$种可能的结果，若在某次试验中$A_k$出现，则认为它出现的概率最高。怎么做到最高？就是把这个事件出现的概率表达式写出来，然后把观察值代入，就只剩未知参数。对未知参数求导（多个未知参数就求偏导），极大值处就是它的最高概率。

**<u>例2</u>**  设总体$X\sim B(1,p)$（那么未知参数就是$p$），从中抽取样本$X_1,X_2,...,X_n$，求参数$p$的极大似然估计。

**解**：$X$的分布律为，

$$
P\{X=x\}=p^x+(1-p)^{1-x},x=0,1
$$

设样本观察值为$x_1,x_2,...,x_n$，则它发生的概率为：

$$
L(p)=\prod_{i=1}^np^{x_i}(1-p)^{1-x_i},x_i=0,1
$$

因此

$$
\ln L(p)=\ln p\sum_{i=1}^nx_i+\ln (1-p)\bigg(n-\sum_{i=1}^nx_i\bigg)
$$

认为这时概率$L(p)$极大，求极大值

$$
\frac{d\ln L(p)}{dp}=0
$$

解得

$$
\hat p=\frac{1}{n}\sum_{i=1}^nx_i=\bar x
$$

因此$p$的极大似然估计量为

$$
\hat p=\frac{1}{n}\sum_{i=1}^nX_i=\bar X
$$

*注：可以发现它与矩估计量是相同的。*

<u>总结</u>：

1. 写出总体概率，根据样本观察值得到样本概率$L(x_1,x_2,...,x_n;\theta_1,\theta_2,...,\theta_m)$；
2. 对各参数求偏导得到极大值点，即为极大似然估计。

## 7.2  估计量的评选标准

> 对于同一个参数，不同估计方法得出的估计量可能不同（而且原则上任何统计量都可以作为未知参数的估计量）。因此我们要评价某种方法下的估计量到底好不好。考虑估计的性质：无偏性，有效性，相合性（一致性）。

### 7.2.1  无偏性

用估计量$\hat \theta =\theta (X_1,X_2,...,X_n)$来估计参数$\theta$，如果该估计量的数学期望$E(\hat \theta)$存在，且$\forall \theta$，$E(\hat \theta )=\theta$，则称$\hat \theta $为$\theta$的**无偏估计量**。

用下面这道例题来说明无偏估计的概念，同时也说明为什么前面样本方差表达式的分母是$n-1$而不是$n$。

**<u>例3</u>**  设总体$X$均值为$\mu$，方差为$\sigma^2$，均未知。证明$\sigma^2$的估计量$\hat \sigma^2=\frac{1}{n}\sum_{i=1}^n(X_i-\bar X)^2$是有偏的，并给出其无偏估计。

**证明**：$\hat \sigma^2=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X^2=A_2-\bar X^2$，因此求$E(A_2)$与$E(\bar X^2)$

$$
E(A_2)=\sigma^2+\mu^2
$$

$$
E(\bar X^2)=D(\bar X)+E^2(\bar X)=\frac{\sigma^2}{n}+\mu^2
$$

故

$$
E(\hat \sigma^2)=E(A_2)-E(\bar X^2)=\frac{n-1}{n}\sigma^2\neq \sigma^2
$$

因此该估计是有偏的。将其无偏化

$$
E(\frac{n}{n-1}\hat \sigma^2)=\sigma^2
$$

因此定义

$$
S^2=\frac{n}{n-1}\hat \sigma^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X)^2
$$

作为方差$\sigma^2$的估计量。

### 7.2.2  有效性

设$\hat \theta_1=\hat \theta_1(X_1,X_2,...,X_n)$和$\hat \theta_2=\hat \theta_2(X_1,X_2,...,X_n)$是对$\theta$的两个无偏估计量。如果$D(\hat \theta_1)\leq D(\hat \theta_2)$（意为$\hat \theta_1$的观察值在真值$\theta$的附近更密集），则称$\hat \theta_1$较$\hat \theta_2$**有效**。

### 7.2.3  相合性（一致性）

设$\hat \theta=\hat \theta(X_1,X_2,...,X_n)$是对$\theta$的估计量，如果$\forall \theta$，当$n\to \infty$时$\hat \theta_n\stackrel{P}{\to}\theta$，则称$\hat \theta$是$\theta$的一致估计。